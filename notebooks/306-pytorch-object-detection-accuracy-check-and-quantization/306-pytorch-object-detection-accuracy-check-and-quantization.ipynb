{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af86d01b",
   "metadata": {
    "id": "JwEAhQVzkAwA"
   },
   "source": [
    "# Quantize the Ultralytics Yolov5 model and check accuracy drop by OpenVINO POT API\n",
    "\n",
    "This tutorial demonstrates step-by-step instructions to perform the model quantization by OpenVINO [Post-Training Optimization Tool (POT)](https://docs.openvino.ai/latest/pot_introduction.html), and to compare model accuracy between FP32 precision and quantized INT8 precision and to show a demo of running model inference based Ultralytics sample with OpenVINO 2022.1 backend.\n",
    "\n",
    "First, follow [Ultralytics Yolov5](https://github.com/ultralytics/yolov5) project to get Yolov5-m model with OpenVINO Intermediate Representation (IR) formats. Then use OpenVINO [Post-Training Optimization Tool (POT)](https://docs.openvino.ai/latest/pot_introduction.html) API to quantize model based on Ultralytics provided Non-Max Suppression (NMS) processing.\n",
    "\n",
    "OpenVINO POT provides two usages:\n",
    "1. Use API to override the model DataLoader class with custom image/annotation loading and preprocessing; Identify your own class which is inherited from Metric for inference result postprocessing and accuracy calculation part.\n",
    "2. Use POT command tool with [Accuracy Checker](https://github.com/openvinotoolkit/open_model_zoo/blob/master/tools/accuracy_checker/README.md) provided adapters, pre/postprocessing, metric by configuration file; this normally recommend to use for OpenVINO [Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo) model by omz_quantizer tool.\n",
    "\n",
    "Ultralytics provided data pre/post-processing functions are different from Accuracy Checker provided configuration for Yolov4 version. Thus, for this case, use POT API which customized DataLoader and Metric can make sure the minimum accuracy difference between FP32 and INT8 model during the quantization. \n",
    "\n",
    "Another quantization method with POT command line tool has been introduced in [Intel Resource & Documentation Center](https://cdrdv2.intel.com/v1/dl/getContent/644715). It cut off the model post-processing part and replace with Accuracy Checker provided postprocess of Yolov4 to check accuracy and quantize. The Accuracy result might be different from Ultralytics original postprocessing results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdf8008",
   "metadata": {
    "id": "QB4Yo-rGGLmV"
   },
   "source": [
    "## Preparation\n",
    "\n",
    "### Get Ultralytics Yolov5 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2573d828",
   "metadata": {
    "id": "2ynWRum4iiTz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'yolov5'...\n",
      "remote: Enumerating objects: 12388, done.\u001b[K\n",
      "remote: Counting objects: 100% (8/8), done.\u001b[K\n",
      "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
      "remote: Total 12388 (delta 1), reused 7 (delta 0), pack-reused 12380\u001b[K\n",
      "Receiving objects: 100% (12388/12388), 12.12 MiB | 7.13 MiB/s, done.\n",
      "Resolving deltas: 100% (8544/8544), done.\n",
      "Note: switching to '3752807c0b8af03d42de478fbcbf338ec4546a6c'.\n",
      "\n",
      "You are in 'detached HEAD' state. You can look around, make experimental\n",
      "changes and commit them, and you can discard any commits you make in this\n",
      "state without impacting any branches by switching back to a branch.\n",
      "\n",
      "If you want to create a new branch to retain commits you create, you may\n",
      "do so (now or later) by using -c with the switch command. Example:\n",
      "\n",
      "  git switch -c <new-branch-name>\n",
      "\n",
      "Or undo this operation with:\n",
      "\n",
      "  git switch -\n",
      "\n",
      "Turn off this advice by setting config variable advice.detachedHead to false\n",
      "\n",
      "\u001b[34m\u001b[1mexport: \u001b[0mdata=data/coco128.yaml, weights=['yolov5m/yolov5m.pt'], imgsz=[640], batch_size=1, device=cpu, half=False, inplace=False, train=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=12, verbose=False, workspace=4, nms=False, agnostic_nms=False, topk_per_class=100, topk_all=100, iou_thres=0.45, conf_thres=0.25, include=['openvino']\n",
      "YOLOv5 ðŸš€ v6.1-0-g3752807 torch 1.7.1+cpu CPU\n",
      "\n",
      "Downloading https://github.com/ultralytics/yolov5/releases/download/v6.1/yolov5m.pt to yolov5m/yolov5m.pt...\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40.8M/40.8M [00:04<00:00, 8.92MB/s]\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 290 layers, 21172173 parameters, 0 gradients\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from yolov5m/yolov5m.pt with output shape (1, 25200, 85) (42.8 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.9.0...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success, saved as yolov5m/yolov5m.onnx (85.1 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mOpenVINO:\u001b[0m starting export with openvino 2022.1.0-7019-cdb9bec7210-releases/2022/1...\n",
      "\u001b[34m\u001b[1mOpenVINO:\u001b[0m export success, saved as yolov5m/yolov5m_openvino_model/ (85.5 MB)\n",
      "\n",
      "Export complete (11.74s)\n",
      "Results saved to \u001b[1m/home/fiona/Documents/projects/openvino_notebooks/notebooks/306-pytorch-object-detection-accuracy-check-and-quantization/yolov5/yolov5m\u001b[0m\n",
      "Detect:          python detect.py --weights yolov5m/yolov5m_openvino_model/\n",
      "PyTorch Hub:     model = torch.hub.load('ultralytics/yolov5', 'custom', 'yolov5m/yolov5m_openvino_model/')\n",
      "Validate:        python val.py --weights yolov5m/yolov5m_openvino_model/\n",
      "Visualize:       https://netron.app\n"
     ]
    }
   ],
   "source": [
    "![ ! -d 'yolov5' ] && git clone https://github.com/ultralytics/yolov5.git -b v6.1 && cd yolov5 && python export.py --weights yolov5m/yolov5m.pt --imgsz 640 --batch-size 1 --include openvino"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b80fe4",
   "metadata": {},
   "source": [
    "### Import Modules of POT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a9303a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from addict import Dict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "sys.path.append(\"./yolov5/\")\n",
    "\n",
    "from yolov5.utils.datasets import create_dataloader\n",
    "from yolov5.utils.general import check_dataset, non_max_suppression, scale_coords, xywh2xyxy, check_yaml,increment_path\n",
    "from yolov5.utils.metrics import ap_per_class\n",
    "from yolov5.val import process_batch\n",
    "\n",
    "from openvino.tools.pot.api import Metric, DataLoader\n",
    "from openvino.tools.pot.engines.ie_engine import IEEngine\n",
    "from openvino.tools.pot.graph import load_model, save_model\n",
    "from openvino.tools.pot.graph.model_utils import compress_model_weights\n",
    "from openvino.tools.pot.pipeline.initializer import create_pipeline\n",
    "from openvino.tools.pot.utils.logger import init_logger, get_logger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b33033e",
   "metadata": {
    "id": "u5xKw0hR0jq6"
   },
   "source": [
    "## Model Quantization by POT API\n",
    "### Create Yolov5 DataLoader class\n",
    "\n",
    "Create a class for Yolov5 dataset and annotation loading which inherit from POT API class DataLoader. Ultralytics Yolov5 training progress require image data nomolize from [0,225] 8 bits integer to [0.0,1.0] 32 bits Float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9600481",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xGKkMRfvi0op",
    "outputId": "4eb1f9af-a4c5-424c-f808-dd9cc2600975"
   },
   "outputs": [],
   "source": [
    "class YOLOv5DataLoader(DataLoader):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        if not isinstance(config, Dict):\n",
    "            config = Dict(config)\n",
    "        super().__init__(config)\n",
    "\n",
    "        self._data_source = config.data_source\n",
    "        self._imgsz = config.imgsz\n",
    "        self._batch_size = 1\n",
    "        self._stride = 32\n",
    "        self._single_cls = config.single_cls\n",
    "        self._pad = 0.5\n",
    "        self._rect = False\n",
    "        self._workers = 1\n",
    "        self._data_loader = self._init_dataloader()\n",
    "        self._data_iter = iter(self._data_loader)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._data_loader.dataset)\n",
    "\n",
    "    def _init_dataloader(self):\n",
    "        dataloader = create_dataloader(self._data_source['val'], imgsz=self._imgsz, batch_size=self._batch_size, stride=self._stride,\n",
    "                                       single_cls=self._single_cls, pad=self._pad, rect=self._rect, workers=self._workers)[0]\n",
    "        return dataloader\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        try:\n",
    "            batch_data = next(self._data_iter)\n",
    "        except StopIteration:\n",
    "            self._data_iter = iter(self._data_loader)\n",
    "            batch_data = next(self._data_iter)\n",
    "\n",
    "        im, target, path, shape = batch_data\n",
    "\n",
    "        im = im.float()  \n",
    "        im /= 255  \n",
    "        nb, _, height, width = im.shape  \n",
    "        img = im.cpu().detach().numpy()\n",
    "        target = target.cpu().detach().numpy()\n",
    "\n",
    "        annotation = dict()\n",
    "        annotation['image_path'] = path\n",
    "        annotation['target'] = target\n",
    "        annotation['batch_size'] = nb\n",
    "        annotation['shape'] = shape\n",
    "        annotation['width'] = width\n",
    "        annotation['height'] = height\n",
    "        annotation['img'] = img\n",
    "\n",
    "        return (item, annotation), img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad92bb9",
   "metadata": {
    "id": "Rhc_7EObUypw"
   },
   "source": [
    "### Create Yolov5 Metric class\n",
    "\n",
    "Create a class to measure the model accuracy by Mean Average Precision (mAP) with COCO dataset predicted result and annotation value. Here both use AP@0.5 and AP@0.5:0.95 as measurement standard.This class should be inherited from POT API Metric class.\n",
    "\n",
    "The COCOMetric.update() function contains post-processing with Non-Max Suppression to sort boxes by score and select the box with highest score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "659aeac7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ipQWpbgQUxoo",
    "outputId": "bbc1734a-c2a2-4261-ed45-264b9e3edd00"
   },
   "outputs": [],
   "source": [
    "class COCOMetric(Metric):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self._metric_dict = {\"AP@0.5\": [], \"AP@0.5:0.95\": []}\n",
    "        self._names = (*self._metric_dict,)\n",
    "        self._stats = []\n",
    "        self._last_stats = []\n",
    "        self._conf_thres = config.conf_thres\n",
    "        self._iou_thres = config.iou_thres\n",
    "        self._single_cls = config.single_cls\n",
    "        self._nc = config.nc\n",
    "        self._class_names = {i:name for i,name in enumerate(config.names)}\n",
    "        self._device = config.device\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        \"\"\" Returns accuracy metric value for the last model output.\n",
    "        Possible format: {metric_name: [metric_values_per_image]}\n",
    "        \"\"\"\n",
    "        mp, mr, map50, map = self._process_stats(self._last_stats)\n",
    "\n",
    "        return {self._names[0]: [map50], self._names[1]: [map]}\n",
    "\n",
    "    @property\n",
    "    def avg_value(self):\n",
    "        \"\"\" Returns accuracy metric value for all model outputs.\n",
    "        Possible format: {metric_name: metric_value}\n",
    "        \"\"\"\n",
    "        mp, mr, map50, map = self._process_stats(self._stats)\n",
    "\n",
    "        return {self._names[0]: map50, self._names[1]: map}\n",
    "\n",
    "    def _process_stats(self, stats):\n",
    "        mp, mr, map50, map = 0.0, 0.0, 0.0, 0.0\n",
    "        stats = [np.concatenate(x, 0) for x in zip(*stats)]  \n",
    "        if len(stats) and stats[0].any():\n",
    "            tp, fp, p, r, f1, ap, ap_class = ap_per_class(*stats, plot=False, save_dir=None, names=self._class_names)\n",
    "            ap50, ap = ap[:, 0], ap.mean(1) \n",
    "            mp, mr, map50, map = p.mean(), r.mean(), ap50.mean(), ap.mean()\n",
    "            np.bincount(stats[3].astype(np.int64), minlength=self._nc)  \n",
    "        else:\n",
    "            torch.zeros(1)\n",
    "\n",
    "        return mp, mr, map50, map\n",
    "\n",
    "    def update(self, output, target):\n",
    "        \"\"\" Calculates and updates metric value\n",
    "        :param output: model output\n",
    "        :param target: annotations\n",
    "        \"\"\"\n",
    "\n",
    "        annotation = target[0][\"target\"]\n",
    "        width = target[0][\"width\"]\n",
    "        height = target[0][\"height\"]\n",
    "        shapes = target[0][\"shape\"]\n",
    "        paths = target[0][\"image_path\"]\n",
    "        im = target[0][\"img\"]\n",
    "\n",
    "        iouv = torch.linspace(0.5, 0.95, 10).to(self._device)  # iou vector for mAP@0.5:0.95\n",
    "        niou = iouv.numel()\n",
    "        seen = 0\n",
    "        stats = []\n",
    "        # NMS\n",
    "        annotation = torch.Tensor(annotation)\n",
    "        annotation[:, 2:] *= torch.Tensor([width, height, width, height]).to(self._device)  # to pixels\n",
    "        lb = []\n",
    "        out = output[0]\n",
    "        out = torch.Tensor(out).to(self._device)\n",
    "        out = non_max_suppression(out, self._conf_thres, self._iou_thres, labels=lb,\n",
    "                                  multi_label=True, agnostic=self._single_cls)\n",
    "        # Metrics\n",
    "        for si, pred in enumerate(out):\n",
    "            labels = annotation[annotation[:, 0] == si, 1:]\n",
    "            nl = len(labels)\n",
    "            tcls = labels[:, 0].tolist() if nl else []  # target class\n",
    "            _, shape = Path(paths[si]), shapes[si][0]\n",
    "            seen += 1\n",
    "\n",
    "            if len(pred) == 0:\n",
    "                if nl:\n",
    "                    stats.append((torch.zeros(0, niou, dtype=torch.bool), torch.Tensor(), torch.Tensor(), tcls))\n",
    "                continue\n",
    "\n",
    "            # Predictions\n",
    "            if self._single_cls:\n",
    "                pred[:, 5] = 0\n",
    "            predn = pred.clone()\n",
    "            scale_coords(im[si].shape[1:], predn[:, :4], shape, shapes[si][1])  # native-space pred\n",
    "\n",
    "            # Evaluate\n",
    "            if nl:\n",
    "                tbox = xywh2xyxy(labels[:, 1:5])  # target boxes\n",
    "                scale_coords(im[si].shape[1:], tbox, shape, shapes[si][1])  # native-space labels\n",
    "                labelsn = torch.cat((labels[:, 0:1], tbox), 1)  # native-space labels\n",
    "                correct = process_batch(predn, labelsn, iouv)\n",
    "            else:\n",
    "                correct = torch.zeros(pred.shape[0], niou, dtype=torch.bool)\n",
    "            stats.append((correct.cpu(), pred[:, 4].cpu(), pred[:, 5].cpu(), tcls))  # (correct, conf, pcls, tcls)\n",
    "            self._stats.append((correct.cpu(), pred[:, 4].cpu(), pred[:, 5].cpu(), tcls))\n",
    "        self._last_stats = stats\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\" Resets metric \"\"\"\n",
    "        self._metric_dict = {\"AP@0.5\": [], \"AP@0.5:0.95\": []}\n",
    "        self._last_stats = []\n",
    "        self._stats = []\n",
    "\n",
    "    def get_attributes(self):\n",
    "        \"\"\"\n",
    "        Returns a dictionary of metric attributes {metric_name: {attribute_name: value}}.\n",
    "        Required attributes: 'direction': 'higher-better' or 'higher-worse'\n",
    "                                                 'type': metric type\n",
    "        \"\"\"\n",
    "        return {self._names[0]: {'direction': 'higher-better',\n",
    "                                 'type': 'AP@0.5'},\n",
    "                self._names[1]: {'direction': 'higher-better',\n",
    "                                 'type': 'AP@0.5:0.95'}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b490241b",
   "metadata": {
    "id": "6JSoEIk60uxV"
   },
   "source": [
    "### Set POT configration\n",
    "\n",
    "Create a function to set configuration of model, engine, dataset, metric and algorithms, which used by POT class and functions.\n",
    "\n",
    "In this case, use \"DefaultQuantization\" for best performance. If change the quantization algorithm to \"AccuracyAwear\", the quantization progress allows keeping accuracy at a predefined range at the cost of performance improvement in case when Default Quantization cannot guarantee it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c1e8029",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_config():\n",
    "    config = Dict()\n",
    "    data_yaml = check_yaml(\"./yolov5/data/coco128.yaml\")\n",
    "    data = check_dataset(data_yaml)\n",
    "\n",
    "    model_config = Dict({\n",
    "        \"model_name\": os.path.splitext(os.path.basename(\"./yolov5/yolov5m/yolov5m_openvino_model/yolov5m.xml\"))[0],\n",
    "        \"model\": \"./yolov5/yolov5m/yolov5m_openvino_model/yolov5m.xml\",\n",
    "        \"weights\": os.path.splitext(\"./yolov5/yolov5m/yolov5m_openvino_model/yolov5m.xml\")[0] + \".bin\"\n",
    "    })\n",
    "\n",
    "    engine_config = Dict({\n",
    "        \"device\": \"CPU\",\n",
    "        \"stat_requests_number\": 8,\n",
    "        \"eval_requests_number\": 8\n",
    "    })\n",
    "\n",
    "    dataset_config = Dict({\n",
    "        \"data_source\": data,\n",
    "        \"imgsz\": 640,\n",
    "        \"single_cls\": True,\n",
    "    })\n",
    "\n",
    "    metric_config = Dict({\n",
    "        \"conf_thres\": 0.001,\n",
    "        \"iou_thres\": 0.65,\n",
    "        \"single_cls\": True,\n",
    "        \"nc\": 1 ,  # if opt.single_cls else int(data['nc']),\n",
    "        \"names\": data[\"names\"],\n",
    "        \"device\": \"cpu\"\n",
    "    })\n",
    "\n",
    "    algorithms = [\n",
    "        {\n",
    "            \"name\": \"DefaultQuantization\",  # or AccuracyAwear\n",
    "            \"params\": {\n",
    "                    \"target_device\": \"CPU\",\n",
    "                    \"preset\": \"performance\",\n",
    "                    \"stat_subset_size\": 300,\n",
    "                    \"model_type\": \"transformer\"\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    config[\"model\"] = model_config\n",
    "    config[\"engine\"] = engine_config\n",
    "    config[\"dataset\"] = dataset_config\n",
    "    config[\"metric\"] = metric_config\n",
    "    config[\"algorithms\"] = algorithms\n",
    "    \n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608279bb",
   "metadata": {
    "id": "FAGmlKQ83ecE"
   },
   "source": [
    "### Run Quantization Pipeline and Accuracy Verification\n",
    "\n",
    "Use 9 steps to quantize model by POT common function progress, the optimized model intermediate representation with collected min-max values will be saved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "738cb5e7",
   "metadata": {
    "id": "QTOoQnSetzQM",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dca236e606e64307912361c3b1b99b0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/6.66M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning '/home/fiona/Documents/projects/openvino_notebooks/notebooks/306-pytorch-object-detection-accuracy-check-and-quantization/datasets/coco128/labels/train2017' images and labels...128 found, 0 missing, 2 empty, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 128/128 [00:00<00:00, 57431.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:openvino.tools.pot.pipeline.pipeline:Evaluation of generated model\n",
      "INFO:openvino.tools.pot.engines.ie_engine:Start inference of 128 images\n",
      "INFO:openvino.tools.pot.engines.ie_engine:13/128 batches are processed in 2.01s\n",
      "INFO:openvino.tools.pot.engines.ie_engine:26/128 batches are processed in 3.86s\n",
      "INFO:openvino.tools.pot.engines.ie_engine:39/128 batches are processed in 5.82s\n",
      "INFO:openvino.tools.pot.engines.ie_engine:52/128 batches are processed in 7.69s\n",
      "INFO:openvino.tools.pot.engines.ie_engine:65/128 batches are processed in 9.40s\n",
      "INFO:openvino.tools.pot.engines.ie_engine:78/128 batches are processed in 11.09s\n",
      "INFO:openvino.tools.pot.engines.ie_engine:91/128 batches are processed in 13.02s\n",
      "INFO:openvino.tools.pot.engines.ie_engine:104/128 batches are processed in 14.80s\n",
      "INFO:openvino.tools.pot.engines.ie_engine:117/128 batches are processed in 16.69s\n",
      "INFO:openvino.tools.pot.engines.ie_engine:Inference finished\n",
      "INFO:__main__:FP32 model metric_results: {'AP@0.5': 0.7628155287017375, 'AP@0.5:0.95': 0.5043693847530953}\n",
      "INFO:openvino.tools.pot.pipeline.pipeline:Inference Engine version:                2022.1.0-7019-cdb9bec7210-releases/2022/1\n",
      "INFO:openvino.tools.pot.pipeline.pipeline:Model Optimizer version:                 2022.1.0-7019-cdb9bec7210-releases/2022/1\n",
      "INFO:openvino.tools.pot.pipeline.pipeline:Post-Training Optimization Tool version: 2022.1.0-7019-cdb9bec7210-releases/2022/1\n",
      "INFO:openvino.tools.pot.statistics.collector:Start computing statistics for algorithms : DefaultQuantization\n",
      "INFO:openvino.tools.pot.statistics.collector:Computing statistics finished\n",
      "INFO:openvino.tools.pot.pipeline.pipeline:Start algorithm: DefaultQuantization\n",
      "INFO:openvino.tools.pot.algorithms.quantization.default.algorithm:Start computing statistics for algorithm : ActivationChannelAlignment\n",
      "INFO:openvino.tools.pot.algorithms.quantization.default.algorithm:Computing statistics finished\n",
      "INFO:openvino.tools.pot.algorithms.quantization.default.algorithm:Start computing statistics for algorithms : MinMaxQuantization,FastBiasCorrection\n",
      "INFO:openvino.tools.pot.algorithms.quantization.default.algorithm:Computing statistics finished\n",
      "INFO:openvino.tools.pot.pipeline.pipeline:Finished: DefaultQuantization\n",
      " ===========================================================================\n",
      "INFO:openvino.tools.pot.pipeline.pipeline:Evaluation of generated model\n",
      "INFO:openvino.tools.pot.engines.ie_engine:Start inference of 128 images\n",
      "INFO:openvino.tools.pot.engines.ie_engine:13/128 batches are processed in 0.79s\n",
      "INFO:openvino.tools.pot.engines.ie_engine:26/128 batches are processed in 1.71s\n",
      "INFO:openvino.tools.pot.engines.ie_engine:39/128 batches are processed in 2.28s\n",
      "INFO:openvino.tools.pot.engines.ie_engine:52/128 batches are processed in 3.02s\n",
      "INFO:openvino.tools.pot.engines.ie_engine:65/128 batches are processed in 3.79s\n",
      "INFO:openvino.tools.pot.engines.ie_engine:78/128 batches are processed in 4.61s\n",
      "INFO:openvino.tools.pot.engines.ie_engine:91/128 batches are processed in 5.26s\n",
      "INFO:openvino.tools.pot.engines.ie_engine:104/128 batches are processed in 6.01s\n",
      "INFO:openvino.tools.pot.engines.ie_engine:117/128 batches are processed in 6.71s\n",
      "INFO:openvino.tools.pot.engines.ie_engine:Inference finished\n",
      "INFO:__main__:Save quantized model in yolov5/yolov5m/yolov5m_openvino_model\n",
      "INFO:__main__:Quantized INT8 model metric_results: {'AP@0.5': 0.7522470819824596, 'AP@0.5:0.95': 0.49075131453809695}\n"
     ]
    }
   ],
   "source": [
    "config = get_config()  # Download dataset and set config\n",
    "init_logger(level='INFO')\n",
    "logger = get_logger(__name__)\n",
    "save_dir = increment_path(Path(\"./yolov5/yolov5m/yolov5m_openvino_model/\"), exist_ok=True)  # increment run\n",
    "save_dir.mkdir(parents=True, exist_ok=True)  # make dir\n",
    "\n",
    "# Step 1: Load the model.\n",
    "model = load_model(config[\"model\"])\n",
    "\n",
    "# Step 2: Initialize the data loader.\n",
    "data_loader = YOLOv5DataLoader(config[\"dataset\"])\n",
    "\n",
    "# Step 3 (Optional. Required for AccuracyAwareQuantization): Initialize the metric.\n",
    "metric = COCOMetric(config[\"metric\"])\n",
    "\n",
    "# Step 4: Initialize the engine for metric calculation and statistics collection.\n",
    "engine = IEEngine(config=config[\"engine\"], data_loader=data_loader, metric=metric)\n",
    "\n",
    "# Step 5: Create a pipeline of compression algorithms.\n",
    "pipeline = create_pipeline(config[\"algorithms\"], engine)\n",
    "\n",
    "metric_results = None\n",
    "\n",
    "# Check the FP32 model accuracy firstly.\n",
    "metric_results_fp32 = pipeline.evaluate(model)\n",
    "\n",
    "logger.info(\"FP32 model metric_results: {}\".format(metric_results_fp32))\n",
    "\n",
    "# Step 6: Execute the pipeline to calculate Min-Max value\n",
    "compressed_model = pipeline.run(model)\n",
    "\n",
    "# Step 7 (Optional):  Compress model weights to quantized precision\n",
    "#                     in order to reduce the size of final .bin file.\n",
    "compress_model_weights(compressed_model)\n",
    "\n",
    "# Step 8: Save the compressed model to the desired path.\n",
    "save_model(compressed_model, os.path.join(os.path.curdir, os.path.join(save_dir,\"optimized\")), config[\"model\"][\"model_name\"])\n",
    "\n",
    "# Step 9 (Optional): Evaluate the compressed model. Print the results.\n",
    "metric_results_i8 = pipeline.evaluate(compressed_model)\n",
    "\n",
    "logger.info(\"Save quantized model in {}\".format(save_dir))\n",
    "logger.info(\"Quantized INT8 model metric_results: {}\".format(metric_results_i8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-deep')\n",
    "fp32_acc = np.array(list(metric_results_fp32.values()))\n",
    "int8_acc = np.array(list(metric_results_i8.values()))\n",
    "x_data = (\"AP@0.5\",\"AP@0.5:0.95\")\n",
    "x_axis = np.arange(len(x_data))\n",
    "fig = plt.figure()\n",
    "fig.patch.set_facecolor('#FFFFFF')\n",
    "fig.patch.set_alpha(0.7)\n",
    "ax = fig.add_subplot(111)\n",
    "plt.bar(x_axis - 0.2, fp32_acc, 0.3, label='FP32')\n",
    "for i in range(0, len(x_axis)):\n",
    "    plt.text(i - 0.3, round(fp32_acc[i],3) + 0.01, str(round(fp32_acc[i],3)),fontweight=\"bold\")\n",
    "plt.bar(x_axis + 0.2, int8_acc, 0.3, label='INT8')\n",
    "for i in range(0, len(x_axis)):\n",
    "    plt.text(i + 0.1, round(int8_acc[i],3) + 0.01, str(round(int8_acc[i],3)),fontweight=\"bold\")\n",
    "plt.xticks(x_axis, x_data)\n",
    "plt.xlabel(\"Accuracy\")\n",
    "plt.title(\"Compare Yolov5 FP32 and INT8 model accuracy\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8de873",
   "metadata": {
    "id": "w3UUduQEGsQm"
   },
   "source": [
    "## Inference Demo Performance Comparison\n",
    "\n",
    "This part shows how to use Ultralytics model detection program [\"detect.py\"](https://github.com/ultralytics/yolov5) to run sync inference with OpenVINO python API for 2 image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44b4f32b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 348
    },
    "id": "1l1JtgeV4Wuw",
    "outputId": "f21c8904-83da-438c-df39-4620bb679554",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['./yolov5m/yolov5m_openvino_model/optimized/yolov5m.xml'], source=data/images, data=data/coco128.yaml, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False\n",
      "YOLOv5 ðŸš€ v6.1-0-g3752807 torch 1.7.1+cpu CPU\n",
      "\n",
      "Loading yolov5m/yolov5m_openvino_model/optimized/yolov5m.xml for OpenVINO inference...\n",
      "image 1/2 /home/fiona/Documents/projects/openvino_notebooks/notebooks/306-pytorch-object-detection-accuracy-check-and-quantization/yolov5/data/images/bus.jpg: 640x640 4 persons, 1 bus, Done. (0.066s)\n",
      "image 2/2 /home/fiona/Documents/projects/openvino_notebooks/notebooks/306-pytorch-object-detection-accuracy-check-and-quantization/yolov5/data/images/zidane.jpg: 640x640 2 persons, 2 ties, Done. (0.051s)\n",
      "Speed: 0.9ms pre-process, 58.7ms inference, 1.7ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns/detect/exp\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!cd yolov5 && python detect.py --weights ./yolov5m/yolov5m_openvino_model/optimized/yolov5m.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c757c451",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "fig2 = plt.figure(figsize=(12, 9))\n",
    "fig2.patch.set_facecolor('#FFFFFF')\n",
    "fig2.patch.set_alpha(0.7)\n",
    "axarr1 = fig2.add_subplot(121)\n",
    "axarr2 = fig2.add_subplot(122)\n",
    "ori = mpimg.imread('./yolov5/data/images/bus.jpg')\n",
    "result = mpimg.imread('./yolov5/runs/detect/exp/bus.jpg')\n",
    "_ = axarr1.imshow(ori)\n",
    "_ = axarr2.imshow(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ed2610",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* [Ultralytics Yolov5](https://github.com/ultralytics/yolov5)\n",
    "* [OpenVINO Post-training Optimization Tool](https://docs.openvino.ai/latest/pot_introduction.html)\n",
    "* [OpenVINO Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo)\n",
    "* [Accuracy Checker](https://github.com/openvinotoolkit/open_model_zoo/blob/master/tools/accuracy_checker/README.md)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('py_notebook_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "60b092b34d71c52e7f3fb23da1f8a2a87b7599b7c79b3bc578eee8fed6ac565c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
